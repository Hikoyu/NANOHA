==========================================
Abridged User's Manual of NANOHA ver.2.0.0
==========================================

Supported Operating System
--------------------------
macOS or Linux (x86 64-bit)


Software Requirements
---------------------
Perl			5.8 or later (64-bit)
(Thread::Queue)		3.07 or later
(List::Util)		1.54 or later
 List::MoreUtils	0.31_01 or later
 Math::Random::MT	1.17 or later
 Number::AnyBase	1.60000 or later
 Inline			0.56 or later
 Inline::C		0.81 or later
 Inline::CPP		0.80 or later
C compiler (gcc/clang)	C++14 compaible
spoa			3.4.0			https://github.com/rvaser/spoa/releases/tag/3.4.0
cmake			3.2 or later		(required for spoa installation)

Note: NANOHA requires Inline, Inline::C, Inline::CPP modules and spoa C++ libraries for XS codes.
      If use pure perl codes instead, these modules and libraries are unnecessary. (but too slow)
      Creating consensus sequences is also disabled. (Only extracting longest sequences is available.)


Installation
------------
Move "nanoha.pl" to any directory in the PATH.

Required Perl modules can be installed via cpan:
```
$ cpan
> reload index
> install List::MoreUtils Math::Random::MT Number::AnyBase Inline Inline::C Inline::CPP
> q
```

Installation of spoa with shared libraries follows below (cmake required):
```
$ wget -O - https://github.com/rvaser/spoa/releases/download/3.4.0/spoa-v3.4.0.tar.gz | tar -xzf -
$ cd spoa-v3.4.0
$ PREFIX=/some/where
$ mkdir build
$ cd build
$ cmake -DCMAKE_INSTALL_PREFIX=$PREFIX -DCMAKE_CXX_FLAGS="-fPIC -Wl,-rpath,$PREFIX/lib" -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON -Dspoa_build_executable=ON -Dspoa_use_simde_openmp=ON ..
$ make
$ make install
```

You can easily install cmake via apt or yum in the Linux system.
In the macOS, MacPorts or Homebrew are helpful to install cmake.
MacPorts	https://www.macports.org
Homebrew	http://brew.sh

XS codes are compiled the first time running NANOHA when setting environment variable NANOHA_XS to true.
Compiled binaries are put on either of '~/.Inline' or './_Inline' directories. 
Making '~/.Inline' directory is recommended because it can be refferred from anywhere you owned.

Environmental variables below effect the compilation.
NANOHA_XS	compile and use XS code
NANOHA_SPOA	compile and use spoa-dependent code for unify subcommand
CC		C compiler name or path
CXX		C++ compiler name or path
SPOA_INC	spoa include direcotry path
SPOA_LIB	spoa library direcotry path

Typical initial compilation procedure:
```
$ mkdir ~/.Inline
$ export NANOHA_XS=1
$ export NANOHA_SPOA=1
$ export SPOA_INC=/some/where/include
$ export SPOA_LIB=/some/where/lib
$ nanoha.pl
```


Quick Start
-----------
General usage is as below:
```
$ nanoha.pl <command> [options] [input] [>output] [2>log]
```

If you omit the command, NANOHA outputs its version and available commands as below:
```
$ nanoha.pl 
nanoha.pl ver.2.0.0

Functions:
  NANOHA is Network-based Assortment of Noisy On-target reads for High-accuracy Alignments.
  This software assorts on-target PacBio/Nanopore reads such as target amplicon sequences.

Usage:
  nanoha.pl <command>

Command:
  assort	Assort sequence reads based on sequence similarity graph
  build		Build sequence similarity graph
  convert	Convert sequence reads from FASTA format to FASTQ format
  dump		Dump k-mer minimizers counts
  sketch	Sketch out sequence reads
  unify		Unify sequence reads in the same cluster
```


[sketch]
```
nanoha.pl sketch

Functions:
  Sketch out sequence reads under specified conditions.

Usage:
  nanoha.pl sketch [options] <STDIN|in1.fq> [in2.fq ...]

Options:
  -3 INT 	Number of bases trimmed from 3' (right) end of each sequence read <0-> [0]
  -5 INT 	Number of bases trimmed from 5' (left) end of each sequence read <0-> [0]
  -k INT 	Size of k-mer <5-15> [15]
  -n INT 	Number of k-mer minimizers to be generated from each sequence read <1-255> [10]
  -o STR 	Output file prefix [nanoha]
  -p INT 	Number of parallel worker threads <1-> [1]
  -s		Use strand-specific sequence reads
  -u INT 	Maximum amount of sequence reads to be loaded <1-4294967295> [4294967295]
  -w		Use 2-byte line feed code (CR+LF) for input files
```

First of all, sketching out input sequence reads.
This command generates k-mer weighted minimizers and converts input sequence reads to the binaries.
Two files with .nsr and .nss extensions are generated.

Examples:
```
$ nanoha.pl sketch -p 4 -k 15 -n 10 -o example input.fastq
```


[build]
```
nanoha.pl build

Functions:
  Build sequence similarity graph under specified conditions.

Usage:
  nanoha.pl build [options] <prefix>

Options:
  -L PATH 	Path to a positive list file of k-mer minimizers
  -l PATH 	Path to a negative list file of k-mer minimizers
  -m INT 	Maximum number of k-mer minimizers to be used from each sequence read <1-255> [255]
  -n INT 	Cutoff number of matched k-mer minimizers <1-255> [1]
  -p INT 	Number of parallel worker threads <1-> [1]
  -v		Use SIMD-vectorized code for LLCS calculation under XS code enabled
```

Second, creating sequence similarity graph.
Each edge weight is calauleted from MinHash method and length of longest common subsequences (LLCS).
A file with .nsg extensions are generated.

Examples:
```
$ nanoha.pl build -p 4 example
```

Note: -v option enable SIMD-vectorized code, which is efficient for LLCS calculation when sequence length is long enough.


[assort]
```
nanoha.pl assort

Functions:
  Assort sequence reads based on sequence similarity graph under specified conditions.

Usage:
  nanoha.pl assort [options] <prefix>

Options:
  -p INT 	Number of parallel worker threads <1-> [1]
  -t INT 	Number of trials per worker thread for clustering sequence reads <1-> [1]
```

Third, creating sequence clusters and assort sequence reads with network-based algorithm.
Sequence clusters are generated via commuity detection method using excess modularity density.
Recommended cutoff depth (cluster size) is calculated by Otsu's method.
Two files with .nsc and .nso extensions are generated.

Examples:
```
$ nanoha.pl assort -p 4 example
```


[unify]
```
nanoha.pl unify

Functions:
  Unify sequence reads in the same cluster under specified conditions.

Usage:
  nanoha.pl unify [options] <prefix> [>out.fa]

Options:
  -a INT 	Maximum amount of sequence reads to be aligned <1-65535> [65535]
  -c STR 	Alignment method to generate consensus sequence reads <local|global|semi-global>
  -d INT 	Cutoff depth (cluster size) to eliminate small clusters <1->
  -e INT 	Gap extension penalty <1-> [1]
  -g INT 	Gap opening penalty <1-> [3]
  -m INT 	Match award <1-> [5]
  -n INT 	Mismatch penalty <0-> [4]
  -p INT 	Number of parallel worker threads <1-> [1]
  -q FLOAT 	Cutoff false discovery rate to eliminate strand-biased clusters <0-1> [0.001]
```

Finally, creating a consensus sequence form each sequence cluster.

Examples:
```
$ nanoha.pl unify -p 4 -c local example >example.unified.fasta
```

Note: Recommended cutoff depth embedded in .nsc file is used by default.


[convert]
```
nanoha.pl convert

Functions:
  Convert sequence reads from FASTA format to FASTQ format under specified conditions.

Usage:
  nanoha.pl convert [options] <STDIN|in1.fa> [in2.fa ...] [>out.fq]

Options:
  -w		Use 2-byte line feed code (CR+LF) for input files
```

Converting sequence reads from FASTA format to FASTQ format with pseudo quality scores.
This command is used for inputting FASTA format sequence reads to sketch command.

Examples:
```
$ nanoha.pl convert input.fasta | nanoha.pl sketch -p 4 -k 15 -n 10 -o example
```


[dump]
```
nanoha.pl dump

Functions:
  Dump k-mer minimizers counts under specified conditions.

Usage:
  nanoha.pl dump [options] <prefix1> [prefix2 ...] [>out.tsv]

Options:
  -l STR 	Comma-separated sample label list
  -n INT 	Use n-th k-mer minimizers <1-> [1]
```

Generate count table of k-mer minimizers.
This table is used for identifying highly observed k-mer minimizers in case-control samples.


Examples:
```
$ nanoha.pl dump case1 case2 control1 control2 >minimizer_count.tsv
```


License
-------
NANOHA is released under the MIT License.
See also LICENSE.


Bug Reporting
-------------
If you find bugs, please let me know.
Email: suzuki.hikoyu@gmail.com
